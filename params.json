{"name":"DoML Archives","tagline":"Archived Notes, Links, Tutorials, Guides","body":"This is the archive for DoML notes.  It will simply be a text dump for now.  First the Intro sequence, then other notes from the emails.\r\n\r\n## New Members Introductory Sequence\r\nI expect there to be about 5 modules in this iteration of the DoML introductory sequence.  \r\n\r\n###First Module: Linear Regression Intro\r\nConsider installing anaconda as a python package manager\r\n\r\nThe videos:\r\nFrom Andrew Ng’s coursera course https://class.coursera.org/ml-003/lecture: all of Chapter 1, Chapter 2: Model Representation through Gradient Descent for learning, and optionally Chapter 4: Multiple Features,  Gradient Descent for Multiple Variables, and Features and Polynomial Regression.\r\nLook through example code in DoML Intro: Module 1\r\n\r\n###Second Module: Overview of Machine Learning\r\nRead \"A few useful things to know about Machine Learning\" (Domingos) \r\n\r\nThe paper:\r\nhttp://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf?utm_source=buffer&utm_campaign=Buffer&utm_content=buffer4697d&utm_medium=twitter\r\n\r\n###Third Module: Logistic Regression\r\nFrom Andrew Ng's coursera course\r\nhttps://class.coursera.org/ml-003/lecture:\r\n\r\nChapter 4: Multiple Features,  Gradient Descent for Multiple Variables, and Features and Polynomial Regression.\r\n\r\nChapter 7 Logistic Regression: all except advanced optimization \r\n\r\n(optional, if you'd like another opinion on it) Chapter 8 Regularization: Regularized Lin Reg, Regl. Log. Reg\r\n\r\nFirst Week: Start implementation of Logistic Regression with Gradient Descent from the starter code in DoML Intro: Module 3, try to get up to implementation with linear models (no feature transformation)\r\n\r\nSecond Week: Try to finish implemenation with feature transforms\r\n\r\n###Fourth Module: Metrics and Cross Validation\r\n\r\n###Fifth Module: First Kaggle Competition\r\n\r\n## Email Digest\r\n\r\n### 2015/01/27\r\nWe’ll talk about the suitability of teaching computers English as a second language[1], or Natural Language Processing, as a domain for collaboration and general involvement with certain supporting Kaggle competitions, such as Sentiment Analysis for Movie Reviews and Job Salary Prediction.  There should still be projects in other domains, but there’s value in collaboration and sharing insights between teams working on problems with similar concepts and potential solution methods.  One goal for this semester in DoML is to foster the community around machine learning, rather than just the dissemination of knowledge; domain sharing and a diverse group of presenters are two good ways to do so.\r\n\r\nWe recommend installing anaconda if you don’t already have a python environment set up, especially for those new to DoML. \r\n\r\n[1] Madison May’s turn of phrase.\r\n\r\n### 2015/01/27\r\nOur eventual goal is ML Expo, an internal Exposition to showcase the work we’ll do this semester.\r\nThe proposed timeline is to complete the introductory sequence within 7 weeks before Spring break, then complete a decent-sized project in the second 7 weeks.\r\n\r\nFor experienced members, we talked about problems and competitions within the NLP domain as the broader topic for collaboration, though some will pursue other topics. \r\n\r\nThe list of topics currently planned is here: http://goo.gl/1MF79S.\r\n\r\nSome relevant kaggle competitions for NLP are here:\r\n\r\nhttp://www.kaggle.com/c/sentiment-analysis-on-movie-reviews\r\n\r\nhttp://www.kaggle.com/c/random-acts-of-pizza\r\n\r\nhttp://www.kaggle.com/c/job-salary-prediction\r\n\r\nhttp://www.kaggle.com/c/word2vec-nlp-tutorial\r\n\r\nFor experience members, our proposed timeline is do work on a project with 1-3 design reviews for each team before the break, then really drill down with the analysis or apply similar tools and methods to another problem in the second half, with 1-2 design reviews + ML Expo.\r\n\r\n### 2015/02/03\r\nToday, we walked through an example pipeline using ipython notebooks, pandas, and sklearn for the \"Random Acts of Pizza\" Kaggle competition.  You can fork the example code from https://github.com/readml/nlp-pizza\r\n\r\nIf you're interested in working on NLP things, make sure you understand tf-idf http://en.wikipedia.org/wiki/Tf%E2%80%93idf\r\n\r\nStart thinking about giving an in-progress presentation to the group about what you're working on within the next few weeks.  The purpose of today's presentation was to give a pipeline example for NLP, but we probably won't have many more like this in the future.\r\nThe plan for future meetings:\r\nPresentations on progress, algorithms, or special topics or areas of expertise; we want everyone to get involved with these; first 15 min - hour\r\n\r\nGroup work, collaboration: rest of the time\r\n\r\nNLP Packages mentioned:\r\nsklearn has some basic vectorization functions, but not much more complex than that\r\n\r\nNLTK, widely used introduction http://www.nltk.org/\r\n\r\nStanford's CoreNLP http://nlp.stanford.edu/software/corenlp.shtml,  search for python.  I haven't used this, but I've read good things about it. I mentioned the treebank, where you can look at how sentences are parsed http://nlp.stanford.edu/sentiment/treebank.html\r\n\r\nspaCy, a new package, apparently very fast; good if you have huge files, but almost no documentation: http://honnibal.github.io/spaCy/index.html\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}
{"name":"DoML Archives","tagline":"Archived Notes, Links, Tutorials, Guides","body":"This is the archive for DoML notes.  It will simply be a text dump for now.  First the Intro sequence, then other notes from the emails.\r\n\r\n## New Members Introductory Sequence\r\nI expect there to be about 5 modules in this iteration of the DoML introductory sequence.  \r\n\r\n###First Module: Linear Regression Intro\r\nConsider installing anaconda as a python package manager\r\n\r\nThe videos:\r\nFrom Andrew Ng’s coursera course https://class.coursera.org/ml-003/lecture: all of Chapter 1, Chapter 2: Model Representation through Gradient Descent for learning, and optionally Chapter 4: Multiple Features,  Gradient Descent for Multiple Variables, and Features and Polynomial Regression.\r\nLook through example code in DoML Intro: Module 1 (or Public:\\DoML Week 1)\r\n\r\n###Second Module: Overview of Machine Learning\r\nRead \"A few useful things to know about Machine Learning\" (Domingos) \r\n\r\nThe paper:\r\nhttp://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf?utm_source=buffer&utm_campaign=Buffer&utm_content=buffer4697d&utm_medium=twitter\r\n\r\nConsult the glossary:\r\nhttps://docs.google.com/spreadsheets/d/1PWMGQ2d3ygvMuLUc7Ce2ZOZWBMPvg4yd2vRkue181GM/edit?usp=sharing\r\n\r\n###Third Module: Logistic Regression\r\nFrom Andrew Ng's coursera course\r\nhttps://class.coursera.org/ml-003/lecture:\r\n\r\nChapter 4: Multiple Features,  Gradient Descent for Multiple Variables, and Features and Polynomial Regression.\r\n\r\nChapter 7 Logistic Regression: all except advanced optimization \r\n\r\n(optional, if you'd like another opinion on it) Chapter 8 Regularization: Regularized Lin Reg, Regl. Log. Reg\r\n\r\nFirst Week: Start implementation of Logistic Regression with Gradient Descent from the starter code in DoML Intro: Module 3 (or Public:\\DoML Week 3, try to get up to implementation with linear models (no feature transformation)\r\n\r\nSecond Week: Try to finish implemenation with feature transforms\r\n\r\n###Fourth Module: Metrics and Cross Validation\r\n\r\n###Fifth Module: First Kaggle Competition\r\n\r\n## Email Digest\r\n\r\n### 2015/01/27\r\nWe’ll talk about the suitability of teaching computers English as a second language[1], or Natural Language Processing, as a domain for collaboration and general involvement with certain supporting Kaggle competitions, such as Sentiment Analysis for Movie Reviews and Job Salary Prediction.  There should still be projects in other domains, but there’s value in collaboration and sharing insights between teams working on problems with similar concepts and potential solution methods.  One goal for this semester in DoML is to foster the community around machine learning, rather than just the dissemination of knowledge; domain sharing and a diverse group of presenters are two good ways to do so.\r\n\r\nWe recommend installing anaconda if you don’t already have a python environment set up, especially for those new to DoML. \r\n\r\n[1] Madison May’s turn of phrase.\r\n\r\n### 2015/01/27\r\nOur eventual goal is ML Expo, an internal Exposition to showcase the work we’ll do this semester.\r\nThe proposed timeline is to complete the introductory sequence within 7 weeks before Spring break, then complete a decent-sized project in the second 7 weeks.\r\n\r\nFor experienced members, we talked about problems and competitions within the NLP domain as the broader topic for collaboration, though some will pursue other topics. \r\n\r\nThe list of topics currently planned is here: http://goo.gl/1MF79S.\r\n\r\nSome relevant kaggle competitions for NLP are here:\r\n\r\nhttp://www.kaggle.com/c/sentiment-analysis-on-movie-reviews\r\n\r\nhttp://www.kaggle.com/c/random-acts-of-pizza\r\n\r\nhttp://www.kaggle.com/c/job-salary-prediction\r\n\r\nhttp://www.kaggle.com/c/word2vec-nlp-tutorial\r\n\r\nFor experience members, our proposed timeline is do work on a project with 1-3 design reviews for each team before the break, then really drill down with the analysis or apply similar tools and methods to another problem in the second half, with 1-2 design reviews + ML Expo.\r\n\r\n### 2015/02/03\r\nToday, we walked through an example pipeline using ipython notebooks, pandas, and sklearn for the \"Random Acts of Pizza\" Kaggle competition.  You can fork the example code from https://github.com/readml/nlp-pizza\r\n\r\nIf you're interested in working on NLP things, make sure you understand tf-idf http://en.wikipedia.org/wiki/Tf%E2%80%93idf\r\n\r\nStart thinking about giving an in-progress presentation to the group about what you're working on within the next few weeks.  The purpose of today's presentation was to give a pipeline example for NLP, but we probably won't have many more like this in the future.\r\nThe plan for future meetings:\r\nPresentations on progress, algorithms, or special topics or areas of expertise; we want everyone to get involved with these; first 15 min - hour\r\n\r\nGroup work, collaboration: rest of the time\r\n\r\nNLP Packages mentioned:\r\nsklearn has some basic vectorization functions, but not much more complex than that\r\n\r\nNLTK, widely used introduction http://www.nltk.org/\r\n\r\nStanford's CoreNLP http://nlp.stanford.edu/software/corenlp.shtml,  search for python.  I haven't used this, but I've read good things about it. I mentioned the treebank, where you can look at how sentences are parsed http://nlp.stanford.edu/sentiment/treebank.html\r\n\r\nspaCy, a new package, apparently very fast; good if you have huge files, but almost no documentation: http://honnibal.github.io/spaCy/index.html\r\n\r\nDoML_email_notes_fall_2014\r\n\r\n###2014/09/16\r\n\r\nPython: Why should you learn it?\r\n\r\nIt’d be great if we have a common platform and language to facilitate group projects.  Python is one of the easiest to pick up, and there are plenty of resources and students on campus to help you with this.  SLAC (Stay Late and Create) is holding a series of Python tutorials this semester; they meet on Wednesday nights in EH2 (look out for further information).\r\n\r\nFinally, this co-curricular is in large part student-driven, so be open with what you want to see happen.  Talk to your friends, talk to Paul, stay after the regular meeting to talk about the direction of group.  One year ago I knew nothing about ML, now I’m making a fool of myself talking about GAs.  You can too!\r\n\r\n\r\n###2014/09/22\r\n\r\nHey DoML,\r\n\r\n \r\n\r\nTomorrow at 6 PM in AC 417, Lindsey, Pratool, and I will be giving a presentation on linear and regularized regression in ML context.  It’d be great if you were there!  This will be a relatively low-intensity introduction to the actual manipulation of data and interpretation of results so that we can get into the machine learning state of mind.  This framing is part of the foundation for later work with progressively more complicated algorithms.\r\n\r\n\r\n###2014/09/23\r\nHey DoML,\r\n\r\n \r\n\r\nThanks to those who were able to make it to tonight’s meeting.  This was the intro to the ML framework, modifying parameters to minimize cost, when linear models are used, and the mechanics of optimization (gradient descent).  If you didn’t get everything, that’s fine, as we’ll continue talking about these topics.  Look over the slides, code, the resources below, and email me or other returning members with questions.  The slides and code are in P:\\DoML Week1.\r\n\r\nThere is an online machine course that covers what we talked about today (video lectures), presented by Andrew Ng (see Ryan’s previous email).\r\n\r\nhttps://class.coursera.org/ml-005/lecture/preview\r\n\r\nChapter II talks about linear regression directly.  Chapters I, III, IV, and VII are all related and could be good to look at.\r\n \r\n\r\nBonus questions, for reflection: How would an intercept be included in the model?  What failure mode did the presented Ridge Regression code demonstrate (it relates to the learning rate)?  What systems can be modeled using linear models?\r\n\r\n ###2014/10/04\r\n\r\n\r\nHey DoML, \r\n\r\nThe glossary for the terms listed is complete for a first pass.  If you get a chance, review the paper presented in the last meeting (http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf) and reference the glossary for terms you didn’t understand.  When I first read the paper, I didn’t think much of the comment on classification hypotheses using the Disjunctive Normal Form, but after rereading and defining it in context, I thought it was a very useful distillation of the idea of overfitting and strong v. weak assumptions (DNF is one of the weakest assumption schemes possible).\r\n\r\nGlossary:\r\n\r\nhttps://docs.google.com/spreadsheets/d/1PWMGQ2d3ygvMuLUc7Ce2ZOZWBMPvg4yd2vRkue181GM/edit?usp=sharing\r\n\r\nThought for reflection:\r\n\r\nMachine Learning – automated search for negative informational entropy.\r\n\r\n###2014/10/07\r\n\r\nHey DoML,\r\n\r\n \r\nWe’ll continue and hopefully finish our implementations next week.  If you’d like to work on it between now and then, feel free to email me or Subhash with any questions.  Also talk to us if you couldn’t make it but would like to know what’s going on.  The starter code is in P:\\DoML_Week3, and the presentation is now there as well.  The forms of the necessary equations are in the attached file.  To translate semi-complex equations into code and see an implementation through are valuable experiences, which is our motivation for having every one work through this algorithm.\r\n\r\n \r\nRyan introduced the first short-term project, a Kaggle competition on city bikeshare usage.  After finishing the logistic regression implementation, you can go on to starting work with Kaggle, scikit learn, and other cool things that Ryan will talk more about at the next meeting.\r\n\r\n###2014/10/14\r\n\r\n\r\nTL;DR: ‘Office Hours.’ A little bit more logistic regression next week, then @ 7 PM, transition to project phase starting with a Kaggle competition. It’ll be a party, everyone’s invited.\r\n\r\nIn today’s meeting we continued with implementation of logistic regression and introduced non-linear decision boundaries.  In the first part of next week’s meeting, we will experiment with non-linear decision boundaries, and play around with several data sets.  Paul will present (or simply give) new optimization methods.  Gradient Descent is a great introduction to optimization because of the simplicity of the update step, but there are much better methods for really any problem formulation.  If you’ve had your share of code and parameter-tuning struggles (which are character building) and have seen the interaction of the different parts of the algorithm, it is appropriate to move on to better algorithms and deconstruct their black boxes as necessary.  During the navigation meeting (immediately after the regular meeting, in which everyone is invited to participate) we concluded that enough time had been spent in the implementation weeds, so sometime soon I’ll put my code in the DoML folder for comparison.  Further, the beginning of next week will have minimal implementation involved – mostly parameter exploration and data set exploration. \r\n\r\n###2014/10/21\r\n\r\n\r\nToday, we wrapped up our talks on logistic regression with non-linear decision boundaries, and launched the Kaggle project boat.  This is a fairly open-ended project time where we’re all working on the same bike sharing modeling Kaggle competition (https://www.kaggle.com/c/bike-sharing-demand) for three weeks.  \r\n\r\n###2014/11/11\r\n\r\n\r\nTonight we gathered in the DoML harbor to have our first learnshare, in which we talked about insights and challenges we had during the warm-up project phase.  We then did some drydock construction in our project ideation and team formation phase.  There are three rough categories of ideas: continue with bikeshare, start another predictive modeling task with given data, such as kaggle or “Driven Data” (data competitions centered on social impact, talk to Ryan about this), or a new domain for those who hunger for adventure on uncharted waters and are willing to spend the time to do it. \r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}
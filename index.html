<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="DoML Archives : Archived Notes, Links, Tutorials, Guides">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>DoML Archives</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/readml/ReadMLSpring2014">View on GitHub</a>

          <h1 id="project_title">DoML Archives</h1>
          <h2 id="project_tagline">Archived Notes, Links, Tutorials, Guides</h2>

            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/readml/ReadMLSpring2014/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/readml/ReadMLSpring2014/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <p>This is the archive for DoML notes.  It will simply be a text dump for now.  First the Intro sequence, then other notes from the emails.</p>

<h2>
<a id="new-members-introductory-sequence" class="anchor" href="#new-members-introductory-sequence" aria-hidden="true"><span class="octicon octicon-link"></span></a>New Members Introductory Sequence</h2>

<p>I expect there to be about 5 modules in this iteration of the DoML introductory sequence.  </p>

<h3>
<a id="first-module-linear-regression-intro" class="anchor" href="#first-module-linear-regression-intro" aria-hidden="true"><span class="octicon octicon-link"></span></a>First Module: Linear Regression Intro</h3>

<p>Consider installing anaconda as a python package manager</p>

<p>The videos:
From Andrew Ng’s coursera course <a href="https://class.coursera.org/ml-003/lecture">https://class.coursera.org/ml-003/lecture</a>: all of Chapter 1, Chapter 2: Model Representation through Gradient Descent for learning, and optionally Chapter 4: Multiple Features,  Gradient Descent for Multiple Variables, and Features and Polynomial Regression.
Look through example code in DoML Intro: Module 1</p>

<h3>
<a id="second-module-overview-of-machine-learning" class="anchor" href="#second-module-overview-of-machine-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Second Module: Overview of Machine Learning</h3>

<p>Read "A few useful things to know about Machine Learning" (Domingos) </p>

<p>The paper:
<a href="http://homes.cs.washington.edu/%7Epedrod/papers/cacm12.pdf?utm_source=buffer&amp;utm_campaign=Buffer&amp;utm_content=buffer4697d&amp;utm_medium=twitter">http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf?utm_source=buffer&amp;utm_campaign=Buffer&amp;utm_content=buffer4697d&amp;utm_medium=twitter</a></p>

<h3>
<a id="third-module-logistic-regression" class="anchor" href="#third-module-logistic-regression" aria-hidden="true"><span class="octicon octicon-link"></span></a>Third Module: Logistic Regression</h3>

<p>From Andrew Ng's coursera course
<a href="https://class.coursera.org/ml-003/lecture">https://class.coursera.org/ml-003/lecture</a>:</p>

<p>Chapter 4: Multiple Features,  Gradient Descent for Multiple Variables, and Features and Polynomial Regression.</p>

<p>Chapter 7 Logistic Regression: all except advanced optimization </p>

<p>(optional, if you'd like another opinion on it) Chapter 8 Regularization: Regularized Lin Reg, Regl. Log. Reg</p>

<p>First Week: Start implementation of Logistic Regression with Gradient Descent from the starter code in DoML Intro: Module 3, try to get up to implementation with linear models (no feature transformation)</p>

<p>Second Week: Try to finish implemenation with feature transforms</p>

<h3>
<a id="fourth-module-metrics-and-cross-validation" class="anchor" href="#fourth-module-metrics-and-cross-validation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Fourth Module: Metrics and Cross Validation</h3>

<h3>
<a id="fifth-module-first-kaggle-competition" class="anchor" href="#fifth-module-first-kaggle-competition" aria-hidden="true"><span class="octicon octicon-link"></span></a>Fifth Module: First Kaggle Competition</h3>

<h2>
<a id="email-digest" class="anchor" href="#email-digest" aria-hidden="true"><span class="octicon octicon-link"></span></a>Email Digest</h2>

<h3>
<a id="20150127" class="anchor" href="#20150127" aria-hidden="true"><span class="octicon octicon-link"></span></a>2015/01/27</h3>

<p>We’ll talk about the suitability of teaching computers English as a second language[1], or Natural Language Processing, as a domain for collaboration and general involvement with certain supporting Kaggle competitions, such as Sentiment Analysis for Movie Reviews and Job Salary Prediction.  There should still be projects in other domains, but there’s value in collaboration and sharing insights between teams working on problems with similar concepts and potential solution methods.  One goal for this semester in DoML is to foster the community around machine learning, rather than just the dissemination of knowledge; domain sharing and a diverse group of presenters are two good ways to do so.</p>

<p>We recommend installing anaconda if you don’t already have a python environment set up, especially for those new to DoML. </p>

<p>[1] Madison May’s turn of phrase.</p>

<h3>
<a id="20150127-1" class="anchor" href="#20150127-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>2015/01/27</h3>

<p>Our eventual goal is ML Expo, an internal Exposition to showcase the work we’ll do this semester.
The proposed timeline is to complete the introductory sequence within 7 weeks before Spring break, then complete a decent-sized project in the second 7 weeks.</p>

<p>For experienced members, we talked about problems and competitions within the NLP domain as the broader topic for collaboration, though some will pursue other topics. </p>

<p>The list of topics currently planned is here: <a href="http://goo.gl/1MF79S">http://goo.gl/1MF79S</a>.</p>

<p>Some relevant kaggle competitions for NLP are here:</p>

<p><a href="http://www.kaggle.com/c/sentiment-analysis-on-movie-reviews">http://www.kaggle.com/c/sentiment-analysis-on-movie-reviews</a></p>

<p><a href="http://www.kaggle.com/c/random-acts-of-pizza">http://www.kaggle.com/c/random-acts-of-pizza</a></p>

<p><a href="http://www.kaggle.com/c/job-salary-prediction">http://www.kaggle.com/c/job-salary-prediction</a></p>

<p><a href="http://www.kaggle.com/c/word2vec-nlp-tutorial">http://www.kaggle.com/c/word2vec-nlp-tutorial</a></p>

<p>For experience members, our proposed timeline is do work on a project with 1-3 design reviews for each team before the break, then really drill down with the analysis or apply similar tools and methods to another problem in the second half, with 1-2 design reviews + ML Expo.</p>

<h3>
<a id="20150203" class="anchor" href="#20150203" aria-hidden="true"><span class="octicon octicon-link"></span></a>2015/02/03</h3>

<p>Today, we walked through an example pipeline using ipython notebooks, pandas, and sklearn for the "Random Acts of Pizza" Kaggle competition.  You can fork the example code from <a href="https://github.com/readml/nlp-pizza">https://github.com/readml/nlp-pizza</a></p>

<p>If you're interested in working on NLP things, make sure you understand tf-idf <a href="http://en.wikipedia.org/wiki/Tf%E2%80%93idf">http://en.wikipedia.org/wiki/Tf%E2%80%93idf</a></p>

<p>Start thinking about giving an in-progress presentation to the group about what you're working on within the next few weeks.  The purpose of today's presentation was to give a pipeline example for NLP, but we probably won't have many more like this in the future.
The plan for future meetings:
Presentations on progress, algorithms, or special topics or areas of expertise; we want everyone to get involved with these; first 15 min - hour</p>

<p>Group work, collaboration: rest of the time</p>

<p>NLP Packages mentioned:
sklearn has some basic vectorization functions, but not much more complex than that</p>

<p>NLTK, widely used introduction <a href="http://www.nltk.org/">http://www.nltk.org/</a></p>

<p>Stanford's CoreNLP <a href="http://nlp.stanford.edu/software/corenlp.shtml">http://nlp.stanford.edu/software/corenlp.shtml</a>,  search for python.  I haven't used this, but I've read good things about it. I mentioned the treebank, where you can look at how sentences are parsed <a href="http://nlp.stanford.edu/sentiment/treebank.html">http://nlp.stanford.edu/sentiment/treebank.html</a></p>

<p>spaCy, a new package, apparently very fast; good if you have huge files, but almost no documentation: <a href="http://honnibal.github.io/spaCy/index.html">http://honnibal.github.io/spaCy/index.html</a></p>
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">DoML Archives maintained by <a href="https://github.com/readml">readml</a></p>
        <p>Published with <a href="http://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
